{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 [Georgina Flesia](georgina.flesia@unc.edu.ar)\n",
    "\n",
    "http://www.famaf.proed.unc.edu.ar/course/view.php?id=470\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning - Teoría bayesiana"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tercer semana Segundo Práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1.  En muchos problemas de clasificacion de patrones se tiene la opcion de asignar un\n",
    " patron a una de $c$ clases, o de rechazarlo como irreconocible. Si el costo de\n",
    " hacer esto ultimo no es demasiado alto, puede ser una accion deseable. Sea\n",
    "\n",
    "  $$\\lambda(\\alpha_i|\\omega_j)=\\left\\{\\begin{array}{ccc}\n",
    " 0& \\ &\\mbox{ si }i=j, \\ i,j=1,2,\\ldots,c\\\\\n",
    " \\lambda_r&&\\mbox{ si }i=c+1\\\\\n",
    " \\lambda_s&&\\mbox{ en otro caso}\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    " donde $\\lambda_r$ es la perdida sufrida por la eleccion de rechazarlo, y $\\lambda_s$ es la\n",
    " p\\'{e}rdida incurrida por cometer un error. Mostrar que el riesgo minimo se obtiene\n",
    " si decidimos $\\omega_i$ si $P(\\omega_i|x)\\geq P (\\omega_j|x)$ para todo $j$, y si $P(\\omega_i|x)\\geq 1-\\frac{\\lambda_r}{\\lambda_s}$, caso contrario, rechazar. ¿Que sucede si $\\lambda_r= 0$? ¿Que sucede si $\\lambda_r>\\lambda_s$?.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2. Considere el problema de clasificacion con la opcion de rechazo como irreconocible.\n",
    "\n",
    "* Utilice los resultados del ejercicio anterior para demostrar que las siguientes funciones discriminantes son optimas para este tipo de problemas:\n",
    "\n",
    "$$g_i(x)=\\left\\{\\begin{array}{ccc}\n",
    " p(x|\\omega_i)P(\\omega_i)& \\ &\\mbox{ si }i=1,2,\\ldots,c\\\\\n",
    " \\displaystyle\\frac{\\lambda_s-\\lambda_r}{\\lambda_s}\\displaystyle\\sum_{j=1}^cp(x|\\omega_j)P(\\omega_j)&&\\mbox{ si }i=c+1\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "* Grafique esta funcion discriminante y las regiones de decision para el caso del problema unidimensional con dos clases, teniendo\n",
    "\n",
    "    * $x|\\omega_1\\sim \\mathcal{N}(1, 1)$,\n",
    "    *  $x|\\omega_2\\sim \\mathcal{N}(-1,1)$,\n",
    "    * $P(\\omega_1) =P(\\omega_2)=\\displaystyle\\frac12$, y\n",
    "    * $\\displaystyle\\frac{\\lambda_r}{\\lambda_s}=\\displaystyle\\frac14$\n",
    "\n",
    "* Describa cualitativamente lo que sucede cuando $\\displaystyle\\frac{\\lambda_r}{\\lambda_s}$ se incrementa de $0$ a $1$.\n",
    "* Repita el procedimiento para el caso\n",
    "\n",
    "    * $x|\\omega_1\\sim \\mathcal{N}(1, 1)$,\n",
    "    * $x|\\omega_2\\sim \\mathcal{N}\\left(0,\\displaystyle\\frac14\\right)$,\n",
    "    * $P(\\omega_1)=\\displaystyle\\frac13$, $P(\\omega_2)=\\displaystyle\\frac23$, y\n",
    "    * $\\displaystyle\\frac{\\lambda_r}{\\lambda_s}=\\displaystyle\\frac12$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 3.  Tres distribuciones , la Gaussiana , la uniforme y la distribucon triangular tienen media cero y desviacion estandar $\\sigma$. Calcule sus entropias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ejercicio 4. \n",
    "Considerar la frontera de decision de Bayes para el caso de clasificacion en dos categorias con $d$ dimensiones.\n",
    "* Demostrar que para cualquier hipercubo arbitrario de $d$ dimensiones, existen distribuciones normales tales que $x|\\omega_i\\sim \\mathcal{N}(\\mu_i,\\Sigma_i)$ y distribuciones a priori $P(\\omega_i)$, $i = 1, 2$, que poseen a este hipercubo como frontera de decisi\\'{o}n de Bayes.\n",
    "* Se mantiene esto si las a priori se mantienen fijas y no nulas, por ejemplo, $P (\\omega_1)=P (\\omega_2)=\\displaystyle\\frac12$?.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ejercicio 5. \n",
    "Suponga que se conocen exactamente las distribuciones $p(x|\\omega_i)$ y a priori $P(\\omega_i)$ en un espacio de caracteristicas $d$ dimensional.\n",
    "\n",
    "* Pruebe que el verdadero error no puede reducirse si uno proyecta las distribuciones en un espacio de menor dimension y luego clasifica.\n",
    "* A pesar de esto, sugiera por que en reconocimiento de patrones usualmente se busca reducir la dimension del espacio de caracteristicas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 6. \n",
    "\n",
    "* Escriba un procedimiento para generar muestras aleatorias de acuerdo a una distribuci\\'{o}n normal\n",
    "$\\mathcal{N}_d(\\mu,\\Sigma)$.\n",
    "* Escriba un procedimiento que calcule la funci\\'{o}n discriminante (de la forma dada en la ecuacion 47) para una distribucion normal dada y probabilidad a priori $P(\\omega_i)$.\n",
    "* Escriba un procedimiento que calcule la distancia eucl\\'{\\i}dea entre dos puntos arbitrarios.\n",
    "* Escriba un procedimiento que calcule la distancia de Mahalanobis entre la media $\\mu$ y un punto arbitrario $x$, dada la matriz de covarianza $\\Sigma$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ejercicio 7. Utilice su clasificador del problema 2(b) para clasificar las 10 muestras de\n",
    " la tabla del libro de la siguiente manera. Suponga que las distribuciones subyacentes son normales.\n",
    "\n",
    "*  Supongamos que las probabilidades a priori de las dos primeras categor\\'{\\i}as son iguales ($P(\\omega_1)=P(\\omega_2)=\\displaystyle\\frac12$ y $P(\\omega_3)= 0$) y el diseño de un clasificador dicotomico para estas dos categor\\'{\\i}as utilizando s\\'{o}lo el valor de caracter\\'{\\i}stica $x_1$.\n",
    "* Determinar el error de entrenamiento emp\\'{\\i}rico en sus muestras, es decir, el porcentaje de puntos mal clasificados.\n",
    "* Utilice la cota de Bhattacharyya para acotar el error que obtendr\\'{a}n los  patrones nuevos obtenidos muestreando las distribuciones.\n",
    "* Repita todo lo anterior, pero ahora utilice dos caracter\\'{\\i}sticas, $x_1$ y $x_2$.\n",
    "* Repita usando las tres caracter\\'{\\i}sticas.\n",
    "* Analice sus resultados. En particular, ¿es siempre posible para un conjunto finito de datos que  el error empirico pueda ser mayor al aumentar la dimension de los datos?.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ejercicio 8. Considerar las tres categorias del ejercicio anterior, y asuma que $P(\\omega_i)=\\displaystyle\\frac13$.\n",
    "\n",
    "* ¿Cu\\'{a}l es la distancia de Mahalanobis entre cada uno de los puntos $(1, 2, 1)^T, (5, 3, 2)^T, (0, 0, 0)^T,\n",
    " (1, 0, 0)^T$ y las medias de cada una de las clases.\n",
    "* Clasificar estos puntos.\n",
    "* Suponga que $P(\\omega_1)=0.8$ y $P(\\omega_2)=P(\\omega_3)=0.1$, y clasifique nuevamente los puntos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ejercicio 9. \\ Considere el problema de clasificacion bidimensional en dos categorias con \n",
    "$$x|\\omega_1\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$, $x |\\omega_2\\sim \\mathcal{N}\\left(\\left(\\begin{array}{c}\n",
    " 1\\\\\n",
    " 1\\end{array}\\right),\\mathbf{I}\\right)$, y $P (\\omega_1)=P (\\omega_2)=\\displaystyle\\frac12$$\n",
    "\n",
    "* Calcular la cota de decision de Bayes.\n",
    "* Calcular la cota el error de Bhattacharyya.\n",
    "* Repita los pasos anteriores con las mismas distribuciones a priori, pero con \n",
    "$$x |\\omega_1\\sim \\mathcal{N}\\left(\\mathbf{0},\\left(\\begin{array}{cc}\n",
    " 2&0.5\\\\\n",
    " 0.5&2\\end{array}\\right)\\right)$ y $x |\\omega_2\\sim \\mathcal{N}\\left(\\left(\\begin{array}{c}\n",
    " 1\\\\\n",
    " 1\\end{array}\\right),\\left(\\begin{array}{cc}\n",
    " 5&4\\\\\n",
    " 4&5\\end{array}\\right)\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
